
YARN的产生背景
	JobTracker：
		负责资源管理和作业调度

	TaskTracker：
		定期向JobTracker汇报本节点的健康状况、资源使用情况、Task的执行情况
		接收来自JobTracker的命令：启动、杀死


	存在的问题
		JT是存在单点故障(*****)
		只能跑MR作业
		资源利用率
		运维成本

	2.x推出了YARN的这么一个框架



YARN（RM + NMs）
	ResourceManager
		RM是老大，整个集群同一个时间点只有一个active(HA)
		负责集群资源的统一管理和调度
		启动和监控AM，如果某个AM挂了，RM将在另外一个NM上启动该AM （容错性体现）
		监控NM，接收小弟的心跳信息，一旦某个NM挂了，标志该NM上的任务该如何处理，告诉AM

	NodeManager
		NM是小弟，整个集群中有多个，负责该节点上的管理和使用
		周期性向RM汇报本节点上的资源使用情况，以及container的运行状态
		接收并处理来自于RM的关于container的启停命令
		处理来自AM的命令

	ApplicationMaster
		AM
		这是每个应用程序一个AM	
		数据切分
		为应用程序向RM去申请资源(container)，分配任务
		与NM通信：启停container
		任务容错
		处理RM发过来的命令

	Container
		是对任务运行环境的抽象
		任务运行资源：节点、内存、CPU
		任务启动命令
		所有的任务都是运行在Container中的，AM、MT、RT都是运行在container中的


	Client
		提交作业、杀死作业  
		使用命令就可以完成

YARN工作原理
Client向YARN中提交应用：Jar 配置文件...
RM为该应用分配第一个Container
需要与NM通信，要求该NM的这个container中运行AM
AM向RM注册，Client可以通过RM查询该应用的运行状态；AM为该应用到RM上去申请资源(MT/RT)


YARN容错性
	RM 
		通过Zookeeper来实现HA，避免单点问题
	NM 
		执行失败后，RM将失败任务告诉对应的AM
		由AM去决定如何处理失败的任务
	AM 
		执行失败后，由RM负责重启



历史服务器
mapred-site.xml
<!-- 历史服务器端地址 -->
<property>
	<name>mapreduce.jobhistory.address</name>
	<value>hadoop000:10020</value>
</property>

<!-- 历史服务器web端地址 -->
<property>
	<name>mapreduce.jobhistory.webapp.address</name>
	<value>hadoop000:19888</value>
</property>


yarn-site.xml
	<!-- 开启日志聚集功能 -->
	<property>
		<name>yarn.log-aggregation-enable</name>
		<value>true</value>
	</property>

	<!-- 设置日志聚集服务器地址 -->
	<property>  
		<name>yarn.log.server.url</name>  
		<value>http://hadoop000:19888/jobhistory/logs</value>
	</property>
	
	<!-- 设置日志保留时间为7天 -->
	<property>
		<name>yarn.log-aggregation.retain-seconds</name>
		<value>604800</value>
	</property>




yarn application -list -appStates XXXX
yarn application -kill applicationid
yarn logs -applicationId applicationid










YARN调度器（调度算法）
	FIFO Scheduler
	Capacity Scheduler：Apache Hadoop默认的调度器
	Fair Scheduler





<property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>default,spark</value>
    <description>
      The queues at the this level (root is the root queue).
    </description>
</property>

<property>
	<name>yarn.scheduler.capacity.root.default.capacity</name>
	<value>30</value>
	<description>Default queue target capacity.</description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
    <value>60</value>
    <description>
      The maximum capacity of the default queue. 
    </description>
  </property>

<property>
	<name>yarn.scheduler.capacity.root.spark.capacity</name>
	<value>70</value>
	<description>Default queue target capacity.</description>
</property>

<property>
    <name>yarn.scheduler.capacity.root.spark.maximum-capacity</name>
    <value>80</value>
    <description>
      The maximum capacity of the default queue. 
    </description>
  </property>

<property>
    <name>yarn.scheduler.capacity.root.spark.user-limit-factor</name>
    <value>1</value>
    <description>
      Default queue user limit a percentage from 0.0 to 1.0.
    </description>
  </property>


<property>
    <name>yarn.scheduler.capacity.root.spark.state</name>
    <value>RUNNING</value>
    <description>
      The state of the default queue. State can be one of RUNNING or STOPPED.
    </description>
  </property>


<property>
    <name>yarn.scheduler.capacity.root.spark.acl_submit_applications</name>
    <value>*</value>
    <description>
      The ACL of who can submit jobs to the default queue.
    </description>
  </property>

<property>
    <name>yarn.scheduler.capacity.root.spark.acl_administer_queue</name>
    <value>*</value>
    <description>
      The ACL of who can administer jobs on the default queue.
    </description>
  </property>

  <property>
    <name>yarn.scheduler.capacity.root.spark.acl_application_max_priority</name>
    <value>*</value>
    <description>
      The ACL of who can submit applications with configured priority.
      For e.g, [user={name} group={name} max_priority={priority} default_priority={priority}]
    </description>
  </property>

<property>
     <name>yarn.scheduler.capacity.root.spark.maximum-application-lifetime
     </name>
     <value>-1</value>
     <description>
        Maximum lifetime of an application which is submitted to a queue
        in seconds. Any value less than or equal to zero will be considered as
        disabled.
        This will be a hard time limit for all applications in this
        queue. If positive value is configured then any application submitted
        to this queue will be killed after exceeds the configured lifetime.
        User can also specify lifetime per application basis in
        application submission context. But user lifetime will be
        overridden if it exceeds queue maximum lifetime. It is point-in-time
        configuration.
        Note : Configuring too low value will result in killing application
        sooner. This feature is applicable only for leaf queue.
     </description>
   </property>

<property>
     <name>yarn.scheduler.capacity.root.spark.default-application-lifetime
     </name>
     <value>-1</value>
     <description>
        Default lifetime of an application which is submitted to a queue
        in seconds. Any value less than or equal to zero will be considered as
        disabled.
        If the user has not submitted application with lifetime value then this
        value will be taken. It is point-in-time configuration.
        Note : Default lifetime can't exceed maximum lifetime. This feature is
        applicable only for leaf queue.
     </description>
   </property>


hadoop jar hadoop-mapreduce-examples-3.3.2.jar pi -Dmapreduce.job.priority=5  2 3



